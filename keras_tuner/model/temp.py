from transformers import AutoTokenizer

tokenizer= AutoTokenizer.from_pretrained("google/gemma-2-9b", pad_token="<pad>")
tokens = tokenizer("What is your name?")
print(tokens)

pred = tokenizer.decode([2, 1841, 603, 861, 1503, 235336, 2961, 1503,603, 7473, 235265, 1, 0, 0, 0, 0])

pred = tokenizer.decode([2,1841,603, 861, 1503, 235336, 109, 2926, 1503, 603])
print(pred)


# Golden logits: 

#  [[[-23.75 7.75 -4.1875 ... -11.5 -6.96875 -23.125]
#    [-30 -15 -17.125 ... -26.875 -24.125 -30]
#    [8.875 18 -3.48438 ... 10.75 10.5625 7]
#    [9.6875 16.375 -5.96875 ... 8.625 8.0625 3.98438]
#    [-3.875 15.25 -13.6875 ... 5.40625 4.40625 -5.5625]
#    [-13.9375 13.8125 -19.25 ... -0.03125 -1.40625 -15.125]]]



# Maxtext logits: 


#    [[ 0.64453125 -0.01721191  0.11279297 ... 0.         0.      0.        ]
#    [ 0.63671875  0.06005859  0.22070312 ...  0.          0.      0.        ]
#    [ 0.50390625  0.00939941  0.08642578 ...  0.          0.      0.        ]
#    [ 0.29296875  0.11083984  0.08056641 ...  0.          0.      0.        ]
#    [ 0.31054688  0.10791016  0.08544922 ...  0.          0.      0.        ]
#    [ 0.29882812  0.11230469  0.08203125 ...  0.          0.      0.        ]]